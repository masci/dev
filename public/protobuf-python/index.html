






<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#FFFFFF" />
  
  <title>Protobuf parsing in Python &middot; /dev/ by Massimiliano Pippi</title>
    <meta name="title" content="Protobuf parsing in Python &middot; /dev/ by Massimiliano Pippi" />
  
  
  
  
  
  <script
    type="text/javascript"
    src="http://localhost:1313/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js"
    integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="
  ></script>
  
  
  
  
  
  
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/css/main.bundle.min.2abed97d6210907523664b7e6725ba691bc95b0ee54b6e46cff169dfba68ae29.css"
    integrity="sha256-Kr7ZfWIQkHUjZkt&#43;ZyW6aRvJWw7lS25Gz/Fp37porik="
  />
  
  
  
  
  
  
  
  <meta
    name="description"
    content="
      
        This blog post was hosted in the Datadog Engineering Blog, you can
read it here.
Recently we extended the Datadog Agent to
support extracting additional metrics from Kubernetes using the
kube-state-metrics
service. Metrics are exported through an HTTP API that supports
content negotiation
so that one can choose between having the response body in plain text format or
as a binary stream encoded using Protocol buffers.
Binary formats are generally assumed to be faster and more efficient, but being
Datadog we wanted to see the data and quantify the improvement.  We hope the
results documented here will help save you time and improve performance in your
own code. But before we dive into our findings, let&rsquo;s start with Protocol
buffers 101.
      
    "
  />
  
  
  
  
    <link rel="canonical" href="http://localhost:1313/protobuf-python/" />
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/protobuf-python/">
  <meta property="og:site_name" content="/dev/ by Massimiliano Pippi">
  <meta property="og:title" content="Protobuf parsing in Python">
  <meta property="og:description" content="This blog post was hosted in the Datadog Engineering Blog, you can read it here.
Recently we extended the Datadog Agent to support extracting additional metrics from Kubernetes using the kube-state-metrics service. Metrics are exported through an HTTP API that supports content negotiation so that one can choose between having the response body in plain text format or as a binary stream encoded using Protocol buffers.
Binary formats are generally assumed to be faster and more efficient, but being Datadog we wanted to see the data and quantify the improvement. We hope the results documented here will help save you time and improve performance in your own code. But before we dive into our findings, let’s start with Protocol buffers 101.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="writing">
    <meta property="article:published_time" content="2017-06-08T00:00:00+01:00">
    <meta property="article:modified_time" content="2017-06-08T00:00:00+01:00">
    <meta property="article:tag" content="Protobuf">
    <meta property="article:tag" content="Python">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Protobuf parsing in Python">
  <meta name="twitter:description" content="This blog post was hosted in the Datadog Engineering Blog, you can read it here.
Recently we extended the Datadog Agent to support extracting additional metrics from Kubernetes using the kube-state-metrics service. Metrics are exported through an HTTP API that supports content negotiation so that one can choose between having the response body in plain text format or as a binary stream encoded using Protocol buffers.
Binary formats are generally assumed to be faster and more efficient, but being Datadog we wanted to see the data and quantify the improvement. We hope the results documented here will help save you time and improve performance in your own code. But before we dive into our findings, let’s start with Protocol buffers 101.">

  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Writings",
    "name": "Protobuf parsing in Python",
    "headline": "Protobuf parsing in Python",
    
    "abstract": "\u003cp\u003e\u003cstrong\u003eThis blog post was hosted in the Datadog Engineering Blog, you can\n\u003ca href=\u0022https:\/\/www.datadoghq.com\/blog\/engineering\/protobuf-parsing-in-python\/\u0022 target=\u0022_blank\u0022 rel=\u0022noreferrer\u0022\u003eread it here\u003c\/a\u003e.\u003c\/strong\u003e\u003c\/p\u003e\n\u003cp\u003eRecently we extended the \u003ca href=\u0022https:\/\/github.com\/DataDog\/dd-agent\u0022 target=\u0022_blank\u0022 rel=\u0022noreferrer\u0022\u003eDatadog Agent\u003c\/a\u003e to\nsupport extracting additional metrics from Kubernetes using the\n\u003ca href=\u0022https:\/\/github.com\/kubernetes\/kube-state-metrics\u0022 target=\u0022_blank\u0022 rel=\u0022noreferrer\u0022\u003ekube-state-metrics\u003c\/a\u003e\nservice. Metrics are exported through an HTTP API that supports\n\u003ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Content_negotiation\u0022 target=\u0022_blank\u0022 rel=\u0022noreferrer\u0022\u003econtent negotiation\u003c\/a\u003e\nso that one can choose between having the response body in plain text format or\nas a binary stream encoded using Protocol buffers.\u003c\/p\u003e\n\u003cp\u003eBinary formats are generally assumed to be faster and more efficient, but being\nDatadog we wanted to see the data and quantify the improvement.  We hope the\nresults documented here will help save you time and improve performance in your\nown code. But before we dive into our findings, let\u0026rsquo;s start with Protocol\nbuffers 101.\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/protobuf-python\/",
    "author" : {
      "@type": "Person",
      "name": "Massimiliano Pippi"
    },
    "copyrightYear": "2017",
    "dateCreated": "2017-06-08T00:00:00\u002b01:00",
    "datePublished": "2017-06-08T00:00:00\u002b01:00",
    
    "dateModified": "2017-06-08T00:00:00\u002b01:00",
    
    "keywords": ["protobuf","python"],
    
    "mainEntityOfPage": "true",
    "wordCount": "1985"
  }
  </script>
    
    <script type="application/ld+json">
    {
   "@context": "https://schema.org",
   "@type": "BreadcrumbList",
   "itemListElement": [
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/",
       "name": "Dev by Massimiliano Pippi",
       "position": 1
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/writing/",
       "name": "Writings",
       "position": 2
     },
     {
       "@type": "ListItem",
       "name": "Protobuf Parsing in Python",
       "position": 3
     }
   ]
 }
  </script>

  
  
    <meta name="author" content="Massimiliano Pippi" />
  
  
    
      <link href="https://github.com/masci" rel="me" />
    
  
  
  







  
  

  
  
</head>
<body
    class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >/dev/ by Massimiliano Pippi</a
  >

    </div>
    
    
      <ul class="flex list-none flex-col text-end sm:flex-row">
        
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/now/"
                  title="What I&#39;m doing Now"
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Now</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/writing/"
                  title="Writings"
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Stuff I write</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/book/"
                  title=""
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Stuff I read</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/talk/"
                  title="Talks"
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Stuff I say</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="https://github.com/masci/dev"
                  title=""
                  target="_blank"
                  >
                    <span
                      class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"
                    ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a
                >
              
            </li>
          
          
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex grow flex-col">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
      <h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        Protobuf parsing in Python
      </h1>
      
        <div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
          





  
  



  

  
  
    
  

  

  

  
    
  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2017-06-08 00:00:00 &#43;0100 &#43;0100">8 June 2017</time><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">10 mins</span>
    

    
    
  </div>

  
  


        </div>
      
      
    </header>
    <section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row">
      
      <div class="min-h-0 min-w-0 max-w-prose grow">
        <p><strong>This blog post was hosted in the Datadog Engineering Blog, you can
<a href="https://www.datadoghq.com/blog/engineering/protobuf-parsing-in-python/" target="_blank" rel="noreferrer">read it here</a>.</strong></p>
<p>Recently we extended the <a href="https://github.com/DataDog/dd-agent" target="_blank" rel="noreferrer">Datadog Agent</a> to
support extracting additional metrics from Kubernetes using the
<a href="https://github.com/kubernetes/kube-state-metrics" target="_blank" rel="noreferrer">kube-state-metrics</a>
service. Metrics are exported through an HTTP API that supports
<a href="https://en.wikipedia.org/wiki/Content_negotiation" target="_blank" rel="noreferrer">content negotiation</a>
so that one can choose between having the response body in plain text format or
as a binary stream encoded using Protocol buffers.</p>
<p>Binary formats are generally assumed to be faster and more efficient, but being
Datadog we wanted to see the data and quantify the improvement.  We hope the
results documented here will help save you time and improve performance in your
own code. But before we dive into our findings, let&rsquo;s start with Protocol
buffers 101.</p>
<h2 id="a-gentle-introduction-to-protocol-buffers" class="relative group">A gentle introduction to Protocol buffers <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#a-gentle-introduction-to-protocol-buffers" aria-label="Anchor">#</a></span></h2><p><a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="noreferrer">Protocol Buffers</a> is a way to
serialize structured data into a binary stream in a fast and efficient manner.
It is designed to be used for inter-machine communication and remote procedure
calls (RPC). This can be used in many different situations, including payloads
for HTTP APIs. To get started you need to learn a simple language that is used
to describe how your data is shaped, but once done a variety of programming
languages can be used to easily read and write Protobuf messages - let’s see a
simple example in Python.</p>
<p>Let’s say we want to provide a list of metrics through an HTTP endpoint: a
metric is something with a name, a string identifier for the type, a floating
point number holding the value and a list of tags, simple strings like
“env:prod” or “role:db” we can use later to filter, aggregate, and compare
results. We could simply print out metric values in plain text with a known
encoding, using some sort of field separator and a newline character to delimit
every entry, something we can implement with a simple <code>format</code>. With Protocol
buffers instead we need to save a text file (we can name it <code>metric.proto</code>) that
contains the following code:</p>
<pre tabindex="0"><code>message Metric {
  required string name = 1;
  required string type = 2;
  required float value = 3;
  repeated string tags = 4;
}
</code></pre><p>Messages in the real world can be way more complex but for the scope of the
article we will try to keep things simple. You can dive deeper browsing the
official docs, namely the
<a href="https://developers.google.com/protocol-buffers/docs/proto" target="_blank" rel="noreferrer">language definition</a>
and the <a href="https://developers.google.com/protocol-buffers/docs/pythontutorial" target="_blank" rel="noreferrer">Python tutorial</a>.</p>
<p>As we mentioned, the <code>.proto</code> file alone is not enough to use the message, we
need some code representing the message itself in a programming language we can
use in our project. A tool called <code>protoc</code> (standing for Protocol buffers
compiler) is provided along with the libraries exactly for this purpose: given
a .proto file in input, it can generate code for messages in several different
languages.</p>
<p>





<figure>
    
    








  
    <picture
      class="mx-auto my-0 rounded-md"
      
    >
      
      
      
      
        <source
          
            
              srcset="http://localhost:1313/protobuf-python/parsing_hu_434f230ad29b6691.webp"
            
          
          sizes="100vw"
          type="image/webp"
        />
      
      <img
        width="500"
        height="363"
        class="mx-auto my-0 rounded-md"
        alt="generating code"
        loading="lazy" decoding="async"
        
          src="http://localhost:1313/protobuf-python/parsing.png"
        
      />
    </picture>
  


</figure>
</p>
<p>We only need the Python code, so after installing protoc we would execute the
command: <code>protoc --python_out=. metric.proto</code></p>
<p>The compiler should generate a Python module named <code>metric_pb2.py</code> that we can
import to serialize data:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8bd5ca">import</span> <span style="color:#f5a97f">metric_pb2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>my_metric <span style="color:#91d7e3;font-weight:bold">=</span> metric_pb2<span style="color:#91d7e3;font-weight:bold">.</span>Metric()
</span></span><span style="display:flex;"><span>my_metric<span style="color:#91d7e3;font-weight:bold">.</span>name <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#a6da95">&#39;sys.cpu&#39;</span>
</span></span><span style="display:flex;"><span>my_metric<span style="color:#91d7e3;font-weight:bold">.</span>type <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#a6da95">&#39;gauge&#39;</span>
</span></span><span style="display:flex;"><span>my_metric<span style="color:#91d7e3;font-weight:bold">.</span>value <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">99.9</span>
</span></span><span style="display:flex;"><span>my_metric<span style="color:#91d7e3;font-weight:bold">.</span>tags<span style="color:#91d7e3;font-weight:bold">.</span>extend([<span style="color:#ed8796">‘</span>my_tag<span style="color:#ed8796">’</span>, <span style="color:#ed8796">‘</span>foo:bar<span style="color:#ed8796">’</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#c6a0f6">with</span> <span style="color:#91d7e3">open</span>(<span style="color:#a6da95">&#39;out.bin&#39;</span>, <span style="color:#a6da95">&#39;wb&#39;</span>) <span style="color:#c6a0f6">as</span> f:
</span></span><span style="display:flex;"><span>    f<span style="color:#91d7e3;font-weight:bold">.</span>write(my_metric<span style="color:#91d7e3;font-weight:bold">.</span>SerializeToString())
</span></span></code></pre></div><p>The code above writes the protobuf stream on a binary file on disk. To read the
data back to Python all we need to do is this:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c6a0f6">with</span> <span style="color:#91d7e3">open</span>(<span style="color:#a6da95">&#39;out.bin&#39;</span>, <span style="color:#a6da95">&#39;rb&#39;</span>) <span style="color:#c6a0f6">as</span> f:
</span></span><span style="display:flex;"><span>    read_metric <span style="color:#91d7e3;font-weight:bold">=</span> metric_pb2<span style="color:#91d7e3;font-weight:bold">.</span>Metric()
</span></span><span style="display:flex;"><span>    read_metric<span style="color:#91d7e3;font-weight:bold">.</span>ParseFromString(f<span style="color:#91d7e3;font-weight:bold">.</span>read())
</span></span><span style="display:flex;"><span>    <span style="color:#6e738d;font-style:italic"># do something with read_metric</span>
</span></span></code></pre></div><h2 id="streaming-multiple-messages" class="relative group">Streaming Multiple Messages <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#streaming-multiple-messages" aria-label="Anchor">#</a></span></h2><p>That is neat but what if we want to encode/decode more than one metric from the
same binary file, or stream a sequence of metrics over a socket? We need a way
to delimit each message during the serialization process, so that processes at
the other end of the wire can determine which chunk of data contains a single
Protocol buffer message: at that point, the decoding part is trivial as we have
already seen.</p>
<p>Unfortunately, Protocol Buffers is not self-delimiting and even if this seems
like a pretty common use case, it’s not obvious how to chain multiple messages
of the same type, one after another, in a binary stream. The <a href="https://developers.google.com/protocol-buffers/docs/techniques?csw=1#streaming" target="_blank" rel="noreferrer">documentation</a>
suggests prepending the message with its size to ease parsing, but the python
implementation does not provide any built in methods for doing this. The Java
implementation however offers methods such as <code>parseDelimitedFrom</code> and
<code>writeDelimitedTo</code> which make this process much simpler. To avoid reinventing
the wheel and for the sake of interoperability, let’s just translate the Java
library’s functionality to Python, writing out the size of the message right
before the message itself. This is also the way kube-state-metrics API chains
multiple messages.</p>
<p>This is quite easy to achieve, except that the Java implementation keeps the
size of the message in a Varint value. Varints are a serialization method that
stores integers in one or more bytes: the smaller the value, the fewer bytes you
need. Even if the concept is quite simple, the implementation in Python is not
trivial but stay with me, there is good news coming.</p>
<p>Protobuf messages are not self-delimited but some of the message fields are.
The idea is always the same: fields are preceded by a Varint containing their
size. That means that somewhere in the Python library there must be some code
that reads and writes Varints - that is what the <code>google.protobuf.internal</code>
package is for:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#8bd5ca">from</span> <span style="color:#f5a97f">google.protobuf.internal.encoder</span> <span style="color:#8bd5ca">import</span> _VarintBytes
</span></span><span style="display:flex;"><span><span style="color:#8bd5ca">from</span> <span style="color:#f5a97f">google.protobuf.internal.decoder</span> <span style="color:#8bd5ca">import</span> _DecodeVarint32
</span></span></code></pre></div><p>This is clearly not intended to be used outside the package itself, but it
seemed useful, so I used it anyway. The code to serialize a stream of messages
would be like:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c6a0f6">with</span> <span style="color:#91d7e3">open</span>(<span style="color:#a6da95">&#39;out.bin&#39;</span>, <span style="color:#a6da95">&#39;wb&#39;</span>) <span style="color:#c6a0f6">as</span> f:
</span></span><span style="display:flex;"><span>    my_tags <span style="color:#91d7e3;font-weight:bold">=</span> (<span style="color:#ed8796">“</span>my_tag<span style="color:#ed8796">”</span>, <span style="color:#ed8796">“</span>foo:bar<span style="color:#ed8796">”</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#c6a0f6">for</span> i <span style="color:#91d7e3;font-weight:bold">in</span> <span style="color:#91d7e3">range</span>(<span style="color:#f5a97f">128</span>):
</span></span><span style="display:flex;"><span>        my_metric <span style="color:#91d7e3;font-weight:bold">=</span> metric_pb2<span style="color:#91d7e3;font-weight:bold">.</span>Metric()
</span></span><span style="display:flex;"><span>        my_metric<span style="color:#91d7e3;font-weight:bold">.</span>name <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#a6da95">&#39;sys.cpu&#39;</span>
</span></span><span style="display:flex;"><span>        my_metric<span style="color:#91d7e3;font-weight:bold">.</span>type <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#a6da95">&#39;gauge&#39;</span>
</span></span><span style="display:flex;"><span>        my_metric<span style="color:#91d7e3;font-weight:bold">.</span>value <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#91d7e3">round</span>(random(), <span style="color:#f5a97f">2</span>)
</span></span><span style="display:flex;"><span>        my_metric<span style="color:#91d7e3;font-weight:bold">.</span>tags<span style="color:#91d7e3;font-weight:bold">.</span>extend(my_tags)
</span></span><span style="display:flex;"><span>        size <span style="color:#91d7e3;font-weight:bold">=</span> my_metric<span style="color:#91d7e3;font-weight:bold">.</span>ByteSize()
</span></span><span style="display:flex;"><span>        f<span style="color:#91d7e3;font-weight:bold">.</span>write(_VarintBytes(size))
</span></span><span style="display:flex;"><span>        f<span style="color:#91d7e3;font-weight:bold">.</span>write(my_metric<span style="color:#91d7e3;font-weight:bold">.</span>SerializeToString())
</span></span></code></pre></div><p>To read back the data we need to take into account the delimiter and the size.
To keep things simple, just read the entire buffer in memory and process the
messages:</p>
<div class="highlight"><pre tabindex="0" style="color:#cad3f5;background-color:#24273a;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#c6a0f6">with</span> <span style="color:#91d7e3">open</span>(<span style="color:#a6da95">&#39;out.bin&#39;</span>, <span style="color:#a6da95">&#39;rb&#39;</span>) <span style="color:#c6a0f6">as</span> f:
</span></span><span style="display:flex;"><span>    buf <span style="color:#91d7e3;font-weight:bold">=</span> f<span style="color:#91d7e3;font-weight:bold">.</span>read()
</span></span><span style="display:flex;"><span>    n <span style="color:#91d7e3;font-weight:bold">=</span> <span style="color:#f5a97f">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#c6a0f6">while</span> n <span style="color:#91d7e3;font-weight:bold">&lt;</span> <span style="color:#91d7e3">len</span>(buf):
</span></span><span style="display:flex;"><span>        msg_len, new_pos <span style="color:#91d7e3;font-weight:bold">=</span> _DecodeVarint32(buf, n)
</span></span><span style="display:flex;"><span>        n <span style="color:#91d7e3;font-weight:bold">=</span> new_pos
</span></span><span style="display:flex;"><span>        msg_buf <span style="color:#91d7e3;font-weight:bold">=</span> buf[n:n<span style="color:#91d7e3;font-weight:bold">+</span>msg_len]
</span></span><span style="display:flex;"><span>        n <span style="color:#91d7e3;font-weight:bold">+=</span> msg_len
</span></span><span style="display:flex;"><span>        read_metric <span style="color:#91d7e3;font-weight:bold">=</span> metric_pb2<span style="color:#91d7e3;font-weight:bold">.</span>Metric()
</span></span><span style="display:flex;"><span>        read_metric<span style="color:#91d7e3;font-weight:bold">.</span>ParseFromString(msg_buf)
</span></span><span style="display:flex;"><span>        <span style="color:#6e738d;font-style:italic"># do something with read_metric</span>
</span></span></code></pre></div><p>As you can see, <code>_DecodeVarint32</code> is so kind to return the new position in the
buffer right after reading the Varint value, so we can easily slice and grab the
chunk containing the message.</p>
<p>At this point you may wondering why bother with Protocol buffers if we need to
introduce a compiler and write Python hacks to do something useful with that.
Let’s try to provide an answer,
<a href="https://www.youtube.com/watch?v=ia-wdEdMUIY" target="_blank" rel="noreferrer">measuring all the things</a>.</p>
<h2 id="benchmarks-anyone" class="relative group">Benchmarks anyone? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#benchmarks-anyone" aria-label="Anchor">#</a></span></h2><p>There are a number of reasons why people need to serialize data: sending
messages between two processes in the same machine, sending them across the
internet, or both - all of these use cases imply a very different set of
requirements. Protocol buffers are clever and efficient but some optimizations
and perks provided by the format are more visible when applied to certain data
formats, or in certains environments: whether this is the right tool or not,
that should be decided on a case by case basis.</p>
<p>Let’s start having a look at the size of the payload produced by encoding a
bunch of messages, both with Protocol buffers and using plain text. For simple
data like our <code>Metric</code>, binary encoding 100k messages takes 3.7Mb on disk that
shrink to about 200 Kb when gzip compressed. If we try to encode the same
message with a naive, streamable, newline-delimited text format like
<code>sys.cpu,gauge,[my_tag,foo:bar],0.99\n</code> that takes 3.4 Mb on disk and about
180 Kb once zipped to store the same 100k entries.</p>
<p>Results are similar with a real world example: a payload returned by the
kube-state-metrics API containing about 60 messages, encoded with Protocol
buffers and gzip compression, takes about 6Kb; the same payload encoded with
the Prometheus text format and gzip compression has pretty much the same size.</p>
<p>





<figure>
    
    








  
    <picture
      class="mx-auto my-0 rounded-md"
      
    >
      
      
      
      
        <source
          
            srcset="http://localhost:1313/protobuf-python/payload_hu_59cb033e08f06681.webp 330w,http://localhost:1313/protobuf-python/payload_hu_cba05f89abee5238.webp 660w
            
              
                ,http://localhost:1313/protobuf-python/payload_hu_119c077e963557e0.webp 971w
              
            
            
              
                ,http://localhost:1313/protobuf-python/payload_hu_119c077e963557e0.webp 971w
              
            "
          
          sizes="100vw"
          type="image/webp"
        />
      
      <img
        width="971"
        height="656"
        class="mx-auto my-0 rounded-md"
        alt="Payload size"
        loading="lazy" decoding="async"
        
          src="http://localhost:1313/protobuf-python/payload_hu_14109223478e570f.png" srcset="http://localhost:1313/protobuf-python/payload_hu_7d9c62029b3094f5.png 330w,http://localhost:1313/protobuf-python/payload_hu_14109223478e570f.png 660w
          
            ,http://localhost:1313/protobuf-python/payload.png 971w
          
          
            ,http://localhost:1313/protobuf-python/payload.png 971w
          "
          sizes="100vw"
        
      />
    </picture>
  


</figure>
</p>
<p>This is somehow expected since strings in Protobuf are utf-8 encoded and our
message is mostly text. If your data looks like our <code>Metric</code> message and you
can use compression, payload size should not be a criterion to choose Protocol
buffers over something else. Or to not choose it.</p>
<p>Speaking of performance, Protocol Buffers should be faster than several other
protocols but in practice, it shows how the Python implementation is extremely
slow. Because of that, pretty much everyone working with Python and Protobuf
uses an optimized version of the library, equipped with a C++ extension; the
current version is not pip installable but it’s fairly easy to build and install
and it provides a huge boost on the performance side.
These are the results using Python 3.5.1 with protobuf 3.1.0 on a MacBook Pro
(13-inch, Early 2015) to encode our <code>Metric</code> type, with and without Protobuf’
cpp extension, compared to the bogus text encoder from a few paragraphs ago.</p>
<p>





<figure>
    
    








  
    <picture
      class="mx-auto my-0 rounded-md"
      
    >
      
      
      
      
        <source
          
            srcset="http://localhost:1313/protobuf-python/encoding_hu_573571f33dbf42d2.webp 330w,http://localhost:1313/protobuf-python/encoding_hu_bb5a122c74ad730c.webp 660w
            
              ,http://localhost:1313/protobuf-python/encoding_hu_804f018e3a18b436.webp 1024w
            
            
              
                ,http://localhost:1313/protobuf-python/encoding_hu_6dd7af4a07b3513a.webp 1164w
              
            "
          
          sizes="100vw"
          type="image/webp"
        />
      
      <img
        width="1164"
        height="710"
        class="mx-auto my-0 rounded-md"
        alt="Encoding performance"
        loading="lazy" decoding="async"
        
          src="http://localhost:1313/protobuf-python/encoding_hu_67a56e1dd5b0e9e9.png" srcset="http://localhost:1313/protobuf-python/encoding_hu_638ef87a12107c16.png 330w,http://localhost:1313/protobuf-python/encoding_hu_67a56e1dd5b0e9e9.png 660w
          
            ,http://localhost:1313/protobuf-python/encoding_hu_4ab5b2afe790d0c1.png 1024w
          
          
            ,http://localhost:1313/protobuf-python/encoding.png 1164w
          "
          sizes="100vw"
        
      />
    </picture>
  


</figure>
</p>
<p>The encoding phase is where Protobuf spends more time. The <code>cpp</code> encoder is not
bad, being able to serialize 10k messages in about 76ms while the pure Python
implementation takes almost half a second. For one million messages the pure
Python protobuf library takes about 40 seconds so it was removed from the chart.</p>
<p>Decoding results are better, with the cpp implementation able to deserialize
10k messages in less than 3ms; using the oversimplified plain text decoder is
like cheating so we don’t plot the values that would be close to zero.</p>
<p>





<figure>
    
    








  
    <picture
      class="mx-auto my-0 rounded-md"
      
    >
      
      
      
      
        <source
          
            srcset="http://localhost:1313/protobuf-python/decoding_hu_1f951840b458f5a2.webp 330w,http://localhost:1313/protobuf-python/decoding_hu_72d4dd4e43d06b07.webp 660w
            
              ,http://localhost:1313/protobuf-python/decoding_hu_c051ac6e04b6afe3.webp 1024w
            
            
              
                ,http://localhost:1313/protobuf-python/decoding_hu_19ca8543c91961a5.webp 1162w
              
            "
          
          sizes="100vw"
          type="image/webp"
        />
      
      <img
        width="1162"
        height="718"
        class="mx-auto my-0 rounded-md"
        alt="Decoding size"
        loading="lazy" decoding="async"
        
          src="http://localhost:1313/protobuf-python/decoding_hu_35d0fd8bd5e78441.png" srcset="http://localhost:1313/protobuf-python/decoding_hu_52a39827e0e9700f.png 330w,http://localhost:1313/protobuf-python/decoding_hu_35d0fd8bd5e78441.png 660w
          
            ,http://localhost:1313/protobuf-python/decoding_hu_97d614ff229fdd6a.png 1024w
          
          
            ,http://localhost:1313/protobuf-python/decoding.png 1162w
          "
          sizes="100vw"
        
      />
    </picture>
  


</figure>
</p>
<p>You can find the code used to run the benchmarks
<a href="https://gist.github.com/masci/7bd5c9d32f4308a227f18de62797b78a" target="_blank" rel="noreferrer">here</a>.</p>
<p>Now that we have a rough idea of what we should expect from Protobuf in terms
of performance, let’s get back to our use case: should we use it to parse
kube-state-metrics? Or should we use the plain text format?</p>
<h2 id="the-short-road-to-protobuf" class="relative group">The (short) road to Protobuf <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#the-short-road-to-protobuf" aria-label="Anchor">#</a></span></h2><p>After playing a bit with Protobuf, it wasn’t obvious whether to choose it or
not to implement the <code>kubernetes_state</code> check. We had to introduce about 5Mb in
binary dependencies to the agent (at this point it should be clear that using
the pure Python version of the library is futile and the C++ runtime does not
come for free) and the benchmarks seemed to show moderate gains over processing
plain text. But trying to look at the entire picture makes things way more clear.</p>
<p>A Python version of the protocol used by the API is publicly available and can
be included in our codebase so we can ignore the overhead in terms of setup and
tooling caused by the compilation process.</p>
<p>In terms of payload size, being the API server capable to compress HTTP
responses, we can choose any of the two formats (Protobuf and plain text)
provided by the kube-state-metrics API without changing the overall performance
of the check.</p>
<p>





<figure>
    
    








  
    <picture
      class="mx-auto my-0 rounded-md"
      
    >
      
      
      
      
        <source
          
            srcset="http://localhost:1313/protobuf-python/decoding60_hu_f695fd1f6bea170b.webp 330w,http://localhost:1313/protobuf-python/decoding60_hu_130c8f9ebb217ef4.webp 660w
            
              ,http://localhost:1313/protobuf-python/decoding60_hu_a458b7ab421421a3.webp 1024w
            
            
              
                ,http://localhost:1313/protobuf-python/decoding60_hu_ca2d2b83ac9f5aac.webp 1102w
              
            "
          
          sizes="100vw"
          type="image/webp"
        />
      
      <img
        width="1102"
        height="694"
        class="mx-auto my-0 rounded-md"
        alt="Decoding size"
        loading="lazy" decoding="async"
        
          src="http://localhost:1313/protobuf-python/decoding60_hu_a6a64503fc2d51e1.png" srcset="http://localhost:1313/protobuf-python/decoding60_hu_51fea5950317868.png 330w,http://localhost:1313/protobuf-python/decoding60_hu_a6a64503fc2d51e1.png 660w
          
            ,http://localhost:1313/protobuf-python/decoding60_hu_f36a2e18932f994f.png 1024w
          
          
            ,http://localhost:1313/protobuf-python/decoding60.png 1102w
          "
          sizes="100vw"
        
      />
    </picture>
  


</figure>
</p>
<p>It’s important to note we access data that is already serialized, meaning that
encoding performance is someone else’s problem; we only need to focus on what’s
the impact of decoding messages on the overall speed of our check. These are the
results of processing 60 metrics from the kube-state-metrics API in both plain
text and protobuf. The text was processed with the Python parser implemented in
the <a href="https://github.com/prometheus/client_python" target="_blank" rel="noreferrer">Prometheus client</a> library.</p>
<p>As you can see, parsing complex data in text format is very different from our
simple metric message: the prometheus parser has to deal with multiple lines,
comments, and nested messages. Putting aside the pure Python version of the
library, parsing the binary payload outperforms plain text by one order of
magnitude, crunching 60 metrics in about 2 milliseconds - the payload transfer
will always be the bottleneck, no matter what.</p>
<p>Even if the Protobuf Python library does not support chained messages out of
the box, the code needed to implement this feature is less than 10 lines. The
data we get is structured, so once the message is parsed we are sure that a
field contains what we expect, meaning less code, less tests, less hacks, less
odds of a regression if the API changes.</p>

      </div>
    </section>
    <footer class="max-w-prose pt-8 print:hidden">
      

      

      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="group flex" href="http://localhost:1313/if-code-is-poetry-then-documentation-is-prose/">
              <span
                class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&larr;</span
                ><span class="ltr:hidden rtl:inline">&rarr;</span></span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >If code is poetry, then documentation is prose</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2014-05-24 00:00:00 &#43;0000 UTC">24 May 2014</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="http://localhost:1313/manage-with-trello/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >How I manage a small team with the help of Trello</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2018-02-03 16:47:03 &#43;0100 CET">3 February 2018</time>
                  
                </span>
              </span>
              <span
                class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&rarr;</span
                ><span class="ltr:hidden rtl:inline">&larr;</span></span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

      </main>
      
        <div
          class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"
          id="to-top"
          hidden="true"
        >
          <a
            href="#the-top"
            class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
            aria-label="Scroll to top"
            title="Scroll to top"
          >
            &uarr;
          </a>
        </div>
      <footer class="py-10 print:hidden">
  
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2025
            Massimiliano Pippi
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://github.com/jpanther/congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    <div class="flex flex-row items-center">
      
      
      
      
        <div
          class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        >
          <button id="appearance-switcher-0" type="button" aria-label="appearance switcher">
            <div
              class="flex h-12 w-12 items-center justify-center dark:hidden"
              title="Switch to dark appearance"
            >
              <span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
            </div>
            <div
              class="hidden h-12 w-12 items-center justify-center dark:flex"
              title="Switch to light appearance"
            >
              <span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
            </div>
          </button>
        </div>
      
    </div>
  </div>
  
  
</footer>

    </div>
  </body>
</html>
