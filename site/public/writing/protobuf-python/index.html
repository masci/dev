

<!DOCTYPE html>
<html lang="en">
    <head>
  <meta charset="utf-8">
  <title> Protobuf parsing in Python &middot; /dev/ </title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://dev.pippi.im//css/bootstrap.min.css" media="screen">
  <link rel="stylesheet" href="https://dev.pippi.im//css/dev.css">
  <link href="https://dev.pippi.im//css/font-awesome.min.css" rel="stylesheet">
  
  
  
  <link href="" rel="alternate" type="application/rss+xml" title="/dev/" />
</head>

    <body>
        <div class="navbar navbar-inverse navbar-fixed-top">
  <div class="container">
    <div class="navbar-header">
      <a href="/" class="navbar-brand">/dev/ by Massimiliano Pippi</a>
    </div>
    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        
      </ul>
    </div>
  </div>
</div>
        <div class="container">
            <h1>Protobuf parsing in Python</h1>
            <h5 id="wc"> 2000 words by masci</h5>
            <div class="row">
                <div class="post col-lg-8">
                    

<p><strong>This blog post was hosted in the Datadog Engineering Blog, you can
<a href="https://www.datadoghq.com/blog/engineering/protobuf-parsing-in-python/">read it here</a>.</strong></p>

<p>Recently we extended the <a href="https://github.com/DataDog/dd-agent">Datadog Agent</a> to
support extracting additional metrics from Kubernetes using the
<a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a>
service. Metrics are exported through an HTTP API that supports
<a href="https://en.wikipedia.org/wiki/Content_negotiation">content negotiation</a>
so that one can choose between having the response body in plain text format or
as a binary stream encoded using Protocol buffers.</p>

<p>Binary formats are generally assumed to be faster and more efficient, but being
Datadog we wanted to see the data and quantify the improvement.  We hope the
results documented here will help save you time and improve performance in your
own code. But before we dive into our findings, let&rsquo;s start with Protocol
buffers 101.</p>

<h2 id="a-gentle-introduction-to-protocol-buffers">A gentle introduction to Protocol buffers</h2>

<p><a href="https://developers.google.com/protocol-buffers/">Protocol Buffers</a> is a way to
serialize structured data into a binary stream in a fast and efficient manner.
It is designed to be used for inter-machine communication and remote procedure
calls (RPC). This can be used in many different situations, including payloads
for HTTP APIs. To get started you need to learn a simple language that is used
to describe how your data is shaped, but once done a variety of programming
languages can be used to easily read and write Protobuf messages - let’s see a
simple example in Python.</p>

<p>Let’s say we want to provide a list of metrics through an HTTP endpoint: a
metric is something with a name, a string identifier for the type, a floating
point number holding the value and a list of tags, simple strings like
“env:prod” or “role:db” we can use later to filter, aggregate, and compare
results. We could simply print out metric values in plain text with a known
encoding, using some sort of field separator and a newline character to delimit
every entry, something we can implement with a simple <code>format</code>. With Protocol
buffers instead we need to save a text file (we can name it <code>metric.proto</code>) that
contains the following code:</p>

<pre><code>message Metric {
  required string name = 1;
  required string type = 2;
  required float value = 3;
  repeated string tags = 4;
}
</code></pre>

<p>Messages in the real world can be way more complex but for the scope of the
article we will try to keep things simple. You can dive deeper browsing the
official docs, namely the
<a href="https://developers.google.com/protocol-buffers/docs/proto">language definition</a>
and the <a href="https://developers.google.com/protocol-buffers/docs/pythontutorial">Python tutorial</a>.</p>

<p>As we mentioned, the <code>.proto</code> file alone is not enough to use the message, we
need some code representing the message itself in a programming language we can
use in our project. A tool called <code>protoc</code> (standing for Protocol buffers
compiler) is provided along with the libraries exactly for this purpose: given
a .proto file in input, it can generate code for messages in several different
languages.</p>

<p><img src="/images/parsing.png" alt="generating code" /></p>

<p>We only need the Python code, so after installing protoc we would execute the
command: <code>protoc --python_out=. metric.proto</code></p>

<p>The compiler should generate a Python module named <code>metric_pb2.py</code> that we can
import to serialize data:</p>

<pre><code class="language-python">import metric_pb2

my_metric = metric_pb2.Metric()
my_metric.name = 'sys.cpu'
my_metric.type = 'gauge'
my_metric.value = 99.9
my_metric.tags.extend([‘my_tag’, ‘foo:bar’])

with open('out.bin', 'wb') as f:
    f.write(my_metric.SerializeToString())
</code></pre>

<p>The code above writes the protobuf stream on a binary file on disk. To read the
data back to Python all we need to do is this:</p>

<pre><code class="language-python">with open('out.bin', 'rb') as f:
    read_metric = metric_pb2.Metric()
    read_metric.ParseFromString(f.read())
    # do something with read_metric
</code></pre>

<h2 id="streaming-multiple-messages">Streaming Multiple Messages</h2>

<p>That is neat but what if we want to encode/decode more than one metric from the
same binary file, or stream a sequence of metrics over a socket? We need a way
to delimit each message during the serialization process, so that processes at
the other end of the wire can determine which chunk of data contains a single
Protocol buffer message: at that point, the decoding part is trivial as we have
already seen.</p>

<p>Unfortunately, Protocol Buffers is not self-delimiting and even if this seems
like a pretty common use case, it’s not obvious how to chain multiple messages
of the same type, one after another, in a binary stream. The <a href="https://developers.google.com/protocol-buffers/docs/techniques?csw=1#streaming">documentation</a>
suggests prepending the message with its size to ease parsing, but the python
implementation does not provide any built in methods for doing this. The Java
implementation however offers methods such as <code>parseDelimitedFrom</code> and
<code>writeDelimitedTo</code> which make this process much simpler. To avoid reinventing
the wheel and for the sake of interoperability, let’s just translate the Java
library’s functionality to Python, writing out the size of the message right
before the message itself. This is also the way kube-state-metrics API chains
multiple messages.</p>

<p>This is quite easy to achieve, except that the Java implementation keeps the
size of the message in a Varint value. Varints are a serialization method that
stores integers in one or more bytes: the smaller the value, the fewer bytes you
need. Even if the concept is quite simple, the implementation in Python is not
trivial but stay with me, there is good news coming.</p>

<p>Protobuf messages are not self-delimited but some of the message fields are.
The idea is always the same: fields are preceded by a Varint containing their
size. That means that somewhere in the Python library there must be some code
that reads and writes Varints - that is what the <code>google.protobuf.internal</code>
package is for:</p>

<pre><code class="language-python">from google.protobuf.internal.encoder import _VarintBytes
from google.protobuf.internal.decoder import _DecodeVarint32
</code></pre>

<p>This is clearly not intended to be used outside the package itself, but it
seemed useful, so I used it anyway. The code to serialize a stream of messages
would be like:</p>

<pre><code class="language-python">with open('out.bin', 'wb') as f:
    my_tags = (“my_tag”, “foo:bar”)
    for i in range(128):
        my_metric = metric_pb2.Metric()
        my_metric.name = 'sys.cpu'
        my_metric.type = 'gauge'
        my_metric.value = round(random(), 2)
        my_metric.tags.extend(my_tags)
        size = my_metric.ByteSize()
        f.write(_VarintBytes(size))
        f.write(my_metric.SerializeToString())
</code></pre>

<p>To read back the data we need to take into account the delimiter and the size.
To keep things simple, just read the entire buffer in memory and process the
messages:</p>

<pre><code class="language-python">with open('out.bin', 'rb') as f:
    buf = f.read()
    n = 0
    while n &lt; len(buf):
        msg_len, new_pos = _DecodeVarint32(buf, n)
        n = new_pos
        msg_buf = buf[n:n+msg_len]
        n += msg_len
        read_metric = metric_pb2.Metric()
        read_metric.ParseFromString(msg_buf)
        # do something with read_metric
</code></pre>

<p>As you can see, <code>_DecodeVarint32</code> is so kind to return the new position in the
buffer right after reading the Varint value, so we can easily slice and grab the
chunk containing the message.</p>

<p>At this point you may wondering why bother with Protocol buffers if we need to
introduce a compiler and write Python hacks to do something useful with that.
Let’s try to provide an answer,
<a href="https://www.youtube.com/watch?v=ia-wdEdMUIY">measuring all the things</a>.</p>

<h2 id="benchmarks-anyone">Benchmarks anyone?</h2>

<p>There are a number of reasons why people need to serialize data: sending
messages between two processes in the same machine, sending them across the
internet, or both - all of these use cases imply a very different set of
requirements. Protocol buffers are clever and efficient but some optimizations
and perks provided by the format are more visible when applied to certain data
formats, or in certains environments: whether this is the right tool or not,
that should be decided on a case by case basis.</p>

<p>Let’s start having a look at the size of the payload produced by encoding a
bunch of messages, both with Protocol buffers and using plain text. For simple
data like our <code>Metric</code>, binary encoding 100k messages takes 3.7Mb on disk that
shrink to about 200 Kb when gzip compressed. If we try to encode the same
message with a naive, streamable, newline-delimited text format like
<code>sys.cpu,gauge,[my_tag,foo:bar],0.99\n</code> that takes 3.4 Mb on disk and about
180 Kb once zipped to store the same 100k entries.</p>

<p>Results are similar with a real world example: a payload returned by the
kube-state-metrics API containing about 60 messages, encoded with Protocol
buffers and gzip compression, takes about 6Kb; the same payload encoded with
the Prometheus text format and gzip compression has pretty much the same size.</p>

<p><img src="/images/payload.png" alt="Payload size" /></p>

<p>This is somehow expected since strings in Protobuf are utf-8 encoded and our
message is mostly text. If your data looks like our <code>Metric</code> message and you
can use compression, payload size should not be a criterion to choose Protocol
buffers over something else. Or to not choose it.</p>

<p>Speaking of performance, Protocol Buffers should be faster than several other
protocols but in practice, it shows how the Python implementation is extremely
slow. Because of that, pretty much everyone working with Python and Protobuf
uses an optimized version of the library, equipped with a C++ extension; the
current version is not pip installable but it’s fairly easy to build and install
and it provides a huge boost on the performance side.
These are the results using Python 3.5.1 with protobuf 3.1.0 on a MacBook Pro
(13-inch, Early 2015) to encode our <code>Metric</code> type, with and without Protobuf’
cpp extension, compared to the bogus text encoder from a few paragraphs ago.</p>

<p><img src="/images/encoding.png" alt="Encoding performance" /></p>

<p>The encoding phase is where Protobuf spends more time. The <code>cpp</code> encoder is not
bad, being able to serialize 10k messages in about 76ms while the pure Python
implementation takes almost half a second. For one million messages the pure
Python protobuf library takes about 40 seconds so it was removed from the chart.</p>

<p>Decoding results are better, with the cpp implementation able to deserialize
10k messages in less than 3ms; using the oversimplified plain text decoder is
like cheating so we don’t plot the values that would be close to zero.</p>

<p><img src="/images/decoding.png" alt="Decoding size" /></p>

<p>You can find the code used to run the benchmarks
<a href="https://gist.github.com/masci/7bd5c9d32f4308a227f18de62797b78a">here</a>.</p>

<p>Now that we have a rough idea of what we should expect from Protobuf in terms
of performance, let’s get back to our use case: should we use it to parse
kube-state-metrics? Or should we use the plain text format?</p>

<h2 id="the-short-road-to-protobuf">The (short) road to Protobuf</h2>

<p>After playing a bit with Protobuf, it wasn’t obvious whether to choose it or
not to implement the <code>kubernetes_state</code> check. We had to introduce about 5Mb in
binary dependencies to the agent (at this point it should be clear that using
the pure Python version of the library is futile and the C++ runtime does not
come for free) and the benchmarks seemed to show moderate gains over processing
plain text. But trying to look at the entire picture makes things way more clear.</p>

<p>A Python version of the protocol used by the API is publicly available and can
be included in our codebase so we can ignore the overhead in terms of setup and
tooling caused by the compilation process.</p>

<p>In terms of payload size, being the API server capable to compress HTTP
responses, we can choose any of the two formats (Protobuf and plain text)
provided by the kube-state-metrics API without changing the overall performance
of the check.</p>

<p><img src="/images/decoding60.png" alt="Decoding size" /></p>

<p>It’s important to note we access data that is already serialized, meaning that
encoding performance is someone else’s problem; we only need to focus on what’s
the impact of decoding messages on the overall speed of our check. These are the
results of processing 60 metrics from the kube-state-metrics API in both plain
text and protobuf. The text was processed with the Python parser implemented in
the <a href="https://github.com/prometheus/client_python">Prometheus client</a> library.</p>

<p>As you can see, parsing complex data in text format is very different from our
simple metric message: the prometheus parser has to deal with multiple lines,
comments, and nested messages. Putting aside the pure Python version of the
library, parsing the binary payload outperforms plain text by one order of
magnitude, crunching 60 metrics in about 2 milliseconds - the payload transfer
will always be the bottleneck, no matter what.</p>

<p>Even if the Protobuf Python library does not support chained messages out of
the box, the code needed to implement this feature is less than 10 lines. The
data we get is structured, so once the message is parsed we are sure that a
field contains what we expect, meaning less code, less tests, less hacks, less
odds of a regression if the API changes.</p>

                    <hr>
                    <ul class="pager">
                         &nbsp;<li class="previous"><a href="https://dev.pippi.im/talk/a-python-and-a-gopher/"> Prev</a></li>
                         &nbsp;<li class="next"><a href="https://dev.pippi.im/writing/manage-with-trello/"> Next</a></li>
                    </ul>
                </div>
                <div class="col-lg-4">
                    <div class="list-group voffset4">
                        <div class="list-group-item">
                            <h4 class="list-group-item-heading">POSTED</h4>
                            <p class="list-group-item-text">Thu, Jun 8, 2017</p>
                        </div>
                        <div class="list-group-item">
                            <h4 class="list-group-item-heading">TAGS</h4>
                            
                            <a href="https://dev.pippi.im//tags/protobuf">protobuf</a>
                            
                            <a href="https://dev.pippi.im//tags/python">python</a>
                            
                        </div>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="post col-lg-8 voffset4">
                    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "dev-pippi" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
                </div>
            </div>
            <footer>
  <div class="row">
    <div class="col-lg-12">
      <hr>
      <small>
        <ul class="list-unstyled">
          <li class="pull-right"><a href="#top">Back to top</a></li>
        </ul>
        <p>
          <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a><br />Contents licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
        <p>
          Original theme by <a href="http://thomaspark.me" rel="nofollow">Thomas Park</a>,
          code released under the <a href="https://github.com/masci/dev/blob/master/LICENSE">MIT License</a>.
        </p>
      </small>
    </div>
  </div>
</footer>
        </div>
        <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43125807-1', 'auto');
  ga('send', 'pageview');

</script>
    </body>
</html>
